{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modules of interest\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import time\n",
    "import datetime\n",
    "import ee as ee\n",
    "ee.Initialize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input the name of the username that serves as the home folder for asset storage\n",
    "usernameFolderString = 'acottam'\n",
    "\n",
    "# Input the name of the project folder inside which all of the assets will be stored\n",
    "# !! You should create this folder immediately under the home asset directory before the script is run\n",
    "projectFolder = 'ETH_Global_Forest_Cover'\n",
    "\n",
    "# Input the name of a folder used to hold the bootstrap collections\n",
    "bootstrapCollFolder = 'Bootstrap_Collections_RegTest'\n",
    "\n",
    "# Input the normal wait time (in seconds) for \"wait and break\" cells\n",
    "normalWaitTime = 5\n",
    "\n",
    "# Input a longer wait time (in seconds) for \"wait and break\" cells\n",
    "longWaitTime = 10\n",
    "\n",
    "# Input the Cloud Storage Bucket that will hold the bootstrap collections when uploading them to Earth Engine\n",
    "# !! This bucket should be pre-created before running this script\n",
    "bucketOfInterest = 'crowther_examples'\n",
    "\n",
    "# Specify the column names where the latitude and longitude information is stored\n",
    "latString = 'Lat'\n",
    "longString = 'Long'\n",
    "\n",
    "# Input the name of the property that holds the CV fold assignment (cross-validation)\n",
    "cvFoldString = 'CV_Fold'\n",
    "\n",
    "# Input the name of the classification property\n",
    "classProperty = 'treecover'\n",
    "\n",
    "# Input a list of the covariates being used\n",
    "covariateList = [\"CHELSA_Annual_Mean_Temperature\",\n",
    "  \"CHELSA_Annual_Precipitation\",\n",
    "  \"EarthEnvTopoMed_Elevation\",\n",
    "  \"SG_Depth_to_bedrock\",\n",
    "  \"CHELSA_Precipitation_Seasonality\",\n",
    "  \"CHELSA_Mean_Temperature_of_Warmest_Quarter\",\n",
    "  \"CHELSA_Precipitation_of_Driest_Quarter\",\n",
    "  \"SG_Sand_Content_000cm\",\n",
    "  \"SG_Sand_Content_005cm\",\n",
    "  \"EarthEnvTopoMed_Northness\",\n",
    "  \"EarthEnvTopoMed_Eastness\"];\n",
    "\n",
    "# Input the name of the folder inside which you want to store the cross validation collections and results\n",
    "cvCollFolder = 'CV_Colls_RegTest'\n",
    "\n",
    "# Input the name of the image collection inside which you'll store the bootstrapped images\n",
    "# AC this doesnt need to be created before hand\n",
    "bootstrapImageColl = 'Bootstrap_Images_RegTest'\n",
    "\n",
    "# Load the composite on which to perform the mapping, and subselect the bands of interest\n",
    "compositeToClassify = ee.Image(\"users/devinrouth/ETH_Composites/CrowtherLab_Composite_30ArcSec\").select(covariateList)\n",
    "\n",
    "# Input the header text that will name each bootstrapped dataset\n",
    "fileNameHeader = 'ForestPotential2_BootstrapColl_RegTest_'\n",
    "\n",
    "# Generate the seeds for bootstrapping\n",
    "seedsToUseForBootstrapping = list(range(1, 6))\n",
    "\n",
    "# Write the name of a local staging area folder for outputted CSV's\n",
    "# holdingFolder = '/Users/DevinRouth/Downloads/ForestPotential2_BootstrapData'\n",
    "holdingFolder = '/home/ubuntu/environment/jupyter/notebooks/crowther/downloads'\n",
    "\n",
    "# Load a geometry to use for the export\n",
    "# exportingGeometry = ee.Geometry.Polygon([-180, 88, 0, 88, 180, 88, 180, -88, 0, -88, -180, -88], None, False);\n",
    "exportingGeometry = ee.Geometry.Polygon(\n",
    "        [[[8.283486006858624, 47.492568637670196],\n",
    "          [8.283486006858624, 47.011581664842936],\n",
    "          [9.162392256858624, 47.011581664842936],\n",
    "          [9.162392256858624, 47.492568637670196]]], None, False);\n",
    "\n",
    "# Determine k for k fold CV\n",
    "k = 4\n",
    "\n",
    "# Input the number of points to use for each bootstrap model\n",
    "# !! This should be chosen carefully, as it will determine the size of the bootstrap collections and models\n",
    "# bootstrapModelSize = 100\n",
    "bootstrapModelSize = 2\n",
    "\n",
    "# Input the model type; i.e., is this a classification (on categorical data) or a regression (on continuous data)?\n",
    "# !! Options should be inputted are 'CLASSIFICATION' or 'REGRESSION'\n",
    "modelType = 'REGRESSION'\n",
    "\n",
    "# Input the title of the CSV that will hold all of the data that has been given a CV fold assignment\n",
    "titleOfCSVWithCVAssignments = \"CV_Fold_Collection_RegTest\"\n",
    "\n",
    "# Input the title of the CV Accuracy Feature Collection\n",
    "cvAccuracyFCNameString = \"CV_Accuracy_FC_RegTest\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the necessary arguments to upload the files to a Cloud Storage bucket\n",
    "# I.e., create bash variables in order to create/check/delete Earth Engine Assets\n",
    "\n",
    "# Specify main bash functions being used\n",
    "# !! You must specify the full path of the executable if the executable is not scoped from root\n",
    "# bashFunction_EarthEngine = '/Users/DevinRouth/Library/Python/3.7/bin/earthengine'\n",
    "bashFunction_EarthEngine = 'earthengine' # tested and works fine!\n",
    "bashFunctionGSUtil = 'gsutil'\n",
    "\n",
    "# Specify the arguments to these functions\n",
    "arglist_preEEUploadTable = ['--no-use_cloud_api','upload','table']\n",
    "arglist_postEEUploadTable = ['--x_column', longString, '--y_column', latString]\n",
    "arglist_preGSUtilUploadFile = ['cp']\n",
    "formattedBucketOI = 'gs://'+bucketOfInterest\n",
    "assetIDStringPrefix = '--asset_id='\n",
    "arglist_CreateCollection = ['--no-use_cloud_api','create','collection']\n",
    "arglist_CreateFolder = ['--no-use_cloud_api','create','folder']\n",
    "arglist_Detect = ['--no-use_cloud_api','asset','info']\n",
    "arglist_Delete = ['--no-use_cloud_api','rm','-r']\n",
    "stringsOfInterest = ['Asset does not exist or is not accessible']\n",
    "\n",
    "# Compose the arguments into lists that can be run via the subprocess module\n",
    "bashCommandList_Detect = [bashFunction_EarthEngine]+arglist_Detect\n",
    "bashCommandList_Delete = [bashFunction_EarthEngine]+arglist_Delete\n",
    "bashCommandList_CreateCollection = [bashFunction_EarthEngine]+arglist_CreateCollection\n",
    "bashCommandList_CreateFolder = [bashFunction_EarthEngine]+arglist_CreateFolder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Forest potential sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 534 ms, sys: 31.9 ms, total: 565 ms\n",
      "Wall time: 571 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Import the raw CSV being bootstrapped\n",
    "rawPointCollection = pd.read_csv('20200303_ForestPotential_Samples.csv',float_precision='round_trip')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['system:index', 'CHELSA_Annual_Mean_Temperature',\n",
      "       'CHELSA_Annual_Precipitation',\n",
      "       'CHELSA_Mean_Temperature_of_Warmest_Quarter',\n",
      "       'CHELSA_Precipitation_Seasonality',\n",
      "       'CHELSA_Precipitation_of_Driest_Quarter', 'EarthEnvTopoMed_Eastness',\n",
      "       'EarthEnvTopoMed_Elevation', 'EarthEnvTopoMed_Northness', 'Lat', 'Long',\n",
      "       'Resolve_Biome', 'SG_Depth_to_bedrock', 'SG_Sand_Content_000cm',\n",
      "       'SG_Sand_Content_005cm', 'shrubcover', 'treecover', '.geo'],\n",
      "      dtype='object')\n",
      "(75772, 18)\n"
     ]
    }
   ],
   "source": [
    "# Print basic information on the csv\n",
    "print(rawPointCollection.columns)\n",
    "print(rawPointCollection.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CHELSA_Annual_Mean_Temperature', 'CHELSA_Annual_Precipitation',\n",
      "       'CHELSA_Mean_Temperature_of_Warmest_Quarter',\n",
      "       'CHELSA_Precipitation_Seasonality',\n",
      "       'CHELSA_Precipitation_of_Driest_Quarter', 'EarthEnvTopoMed_Eastness',\n",
      "       'EarthEnvTopoMed_Elevation', 'EarthEnvTopoMed_Northness', 'Lat', 'Long',\n",
      "       'Resolve_Biome', 'SG_Depth_to_bedrock', 'SG_Sand_Content_000cm',\n",
      "       'SG_Sand_Content_005cm', 'shrubcover', 'treecover'],\n",
      "      dtype='object')\n",
      "(75772, 16)\n",
      "CHELSA_Annual_Mean_Temperature                float64\n",
      "CHELSA_Annual_Precipitation                   float64\n",
      "CHELSA_Mean_Temperature_of_Warmest_Quarter    float64\n",
      "CHELSA_Precipitation_Seasonality              float64\n",
      "CHELSA_Precipitation_of_Driest_Quarter        float64\n",
      "EarthEnvTopoMed_Eastness                      float64\n",
      "EarthEnvTopoMed_Elevation                     float64\n",
      "EarthEnvTopoMed_Northness                     float64\n",
      "Lat                                           float64\n",
      "Long                                          float64\n",
      "Resolve_Biome                                 float64\n",
      "SG_Depth_to_bedrock                           float64\n",
      "SG_Sand_Content_000cm                         float64\n",
      "SG_Sand_Content_005cm                         float64\n",
      "shrubcover                                      int64\n",
      "treecover                                       int64\n",
      "dtype: object\n",
      "CPU times: user 16.5 ms, sys: 46 µs, total: 16.5 ms\n",
      "Wall time: 16.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Remove the \"system:index\" and rename the \".geo\" column to \"geo\" and shuffle the data frame while setting a new index\n",
    "# (to ensure geographic clumps of points are not clumped in anyway)\n",
    "preppedCollection = rawPointCollection.drop(['system:index','.geo'], axis=1).sample(frac=1).reset_index(drop=True)\n",
    "print(preppedCollection.columns)\n",
    "print(preppedCollection.shape)\n",
    "print(preppedCollection.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Collection with NA values included\n",
      "(75772, 16)\n",
      "CHELSA_Annual_Mean_Temperature                3121\n",
      "CHELSA_Annual_Precipitation                   3121\n",
      "CHELSA_Mean_Temperature_of_Warmest_Quarter    3121\n",
      "CHELSA_Precipitation_Seasonality              3121\n",
      "CHELSA_Precipitation_of_Driest_Quarter        3121\n",
      "EarthEnvTopoMed_Eastness                      3094\n",
      "EarthEnvTopoMed_Elevation                     3094\n",
      "EarthEnvTopoMed_Northness                     3094\n",
      "Lat                                              0\n",
      "Long                                             0\n",
      "Resolve_Biome                                  153\n",
      "SG_Depth_to_bedrock                           3093\n",
      "SG_Sand_Content_000cm                         3093\n",
      "SG_Sand_Content_005cm                         3093\n",
      "shrubcover                                       0\n",
      "treecover                                        0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Cleaned Collection with NA values excluded\n",
      "(72524, 16)\n",
      "CHELSA_Annual_Mean_Temperature                0\n",
      "CHELSA_Annual_Precipitation                   0\n",
      "CHELSA_Mean_Temperature_of_Warmest_Quarter    0\n",
      "CHELSA_Precipitation_Seasonality              0\n",
      "CHELSA_Precipitation_of_Driest_Quarter        0\n",
      "EarthEnvTopoMed_Eastness                      0\n",
      "EarthEnvTopoMed_Elevation                     0\n",
      "EarthEnvTopoMed_Northness                     0\n",
      "Lat                                           0\n",
      "Long                                          0\n",
      "Resolve_Biome                                 0\n",
      "SG_Depth_to_bedrock                           0\n",
      "SG_Sand_Content_000cm                         0\n",
      "SG_Sand_Content_005cm                         0\n",
      "shrubcover                                    0\n",
      "treecover                                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Drop NAs\n",
    "print('Original Collection with NA values included')\n",
    "print(preppedCollection.shape)\n",
    "print(preppedCollection.isna().sum())\n",
    "print('\\n')\n",
    "print('Cleaned Collection with NA values excluded')\n",
    "preppedCollection = preppedCollection.dropna(how='any')\n",
    "print(preppedCollection.shape)\n",
    "print(preppedCollection.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# Make a list of the k-fold CV assignments to use\n",
    "kList = list(range(1,k+1))\n",
    "print(kList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CHELSA_Annual_Mean_Temperature', 'CHELSA_Annual_Precipitation',\n",
      "       'CHELSA_Mean_Temperature_of_Warmest_Quarter',\n",
      "       'CHELSA_Precipitation_Seasonality',\n",
      "       'CHELSA_Precipitation_of_Driest_Quarter', 'EarthEnvTopoMed_Eastness',\n",
      "       'EarthEnvTopoMed_Elevation', 'EarthEnvTopoMed_Northness', 'Lat', 'Long',\n",
      "       'Resolve_Biome', 'SG_Depth_to_bedrock', 'SG_Sand_Content_000cm',\n",
      "       'SG_Sand_Content_005cm', 'shrubcover', 'treecover', 'CV_Fold'],\n",
      "      dtype='object')\n",
      "(72524, 17)\n"
     ]
    }
   ],
   "source": [
    "# Add fold assignments to each of the points, stratified by biome\n",
    "preppedCollection[cvFoldString] = (preppedCollection.groupby('Resolve_Biome').cumcount() % k) + 1\n",
    "print(preppedCollection.columns)\n",
    "print(preppedCollection.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4559, 17)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3    1140\n",
       "2    1140\n",
       "1    1140\n",
       "4    1139\n",
       "Name: CV_Fold, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test to ensure that the CV fold assignment has been done correctly\n",
    "test = preppedCollection.loc[preppedCollection['Resolve_Biome'] == 7]\n",
    "print(test.shape)\n",
    "test[cvFoldString].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the CSV to disk and upload it to Earth Engine as a Feature Collection\n",
    "localPathToCVAssignedData = holdingFolder+'/'+titleOfCSVWithCVAssignments+'.csv'\n",
    "preppedCollection.to_csv(localPathToCVAssignedData,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV_Fold_Collection_RegTest uploaded to a GCSB!\n"
     ]
    }
   ],
   "source": [
    "# Format the bash call to upload the file to the Google Cloud Storage bucket\n",
    "gsutilBashUploadList = [bashFunctionGSUtil]+arglist_preGSUtilUploadFile+[localPathToCVAssignedData]+[formattedBucketOI]\n",
    "subprocess.run(gsutilBashUploadList)\n",
    "print(titleOfCSVWithCVAssignments+' uploaded to a GCSB!')\n",
    "\n",
    "# Wait for a short period to ensure the command has been received by the server\n",
    "time.sleep(normalWaitTime/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything is uploaded; moving on...\n"
     ]
    }
   ],
   "source": [
    "# Wait for the GSUTIL uploading process to finish before moving on\n",
    "while not all(x in subprocess.run([bashFunctionGSUtil,'ls',formattedBucketOI],stdout=subprocess.PIPE).stdout.decode('utf-8') for x in [titleOfCSVWithCVAssignments]):\n",
    "    print('Not everything is uploaded...')\n",
    "    time.sleep(normalWaitTime)\n",
    "print('Everything is uploaded; moving on...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload to EE queued!\n"
     ]
    }
   ],
   "source": [
    "# Upload the file into Earth Engine as a table asset\n",
    "assetIDForCVAssignedColl = 'users/'+usernameFolderString+'/'+projectFolder+'/'+titleOfCSVWithCVAssignments\n",
    "earthEngineUploadTableCommands = [bashFunction_EarthEngine]+arglist_preEEUploadTable+[assetIDStringPrefix+assetIDForCVAssignedColl]+[formattedBucketOI+'/'+titleOfCSVWithCVAssignments+'.csv']+arglist_postEEUploadTable\n",
    "subprocess.run(earthEngineUploadTableCommands)\n",
    "print('Upload to EE queued!')\n",
    "\n",
    "# Wait for a short period to ensure the command has been received by the server\n",
    "time.sleep(normalWaitTime/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have jobs running!  2020-04-05 18:36:39\n",
      "You have jobs running!  2020-04-05 18:36:45\n",
      "You have jobs running!  2020-04-05 18:36:50\n",
      "You have jobs running!  2020-04-05 18:36:56\n",
      "You have jobs running!  2020-04-05 18:37:02\n",
      "You have jobs running!  2020-04-05 18:37:07\n",
      "You have jobs running!  2020-04-05 18:37:13\n",
      "You have jobs running!  2020-04-05 18:37:18\n",
      "You have jobs running!  2020-04-05 18:37:23\n",
      "You have jobs running!  2020-04-05 18:37:29\n",
      "You have jobs running!  2020-04-05 18:37:34\n",
      "You have jobs running!  2020-04-05 18:37:40\n",
      "You have jobs running!  2020-04-05 18:37:45\n",
      "You have jobs running!  2020-04-05 18:37:51\n",
      "You have jobs running!  2020-04-05 18:37:56\n",
      "You have jobs running!  2020-04-05 18:38:02\n",
      "You have jobs running!  2020-04-05 18:38:07\n",
      "Moving on...\n"
     ]
    }
   ],
   "source": [
    "# !! Break and wait\n",
    "while any(x in str(ee.batch.Task.list()) for x in ['RUNNING','READY']):\n",
    "    print('You have jobs running! ',datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    time.sleep(normalWaitTime)\n",
    "print('Moving on...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data in GEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the collection with the pre-assigned K-Fold assignments\n",
    "fcOI = ee.FeatureCollection(assetIDForCVAssignedColl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Random Forest Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a selection of random forest classifiers to determine the best model (using features for wrapping, so the entire process can be processed via an export task)\n",
    "rf_VP2 = ee.Feature(ee.Geometry.Point([0,0])).set('cName','rf_VP2','c',ee.Classifier.smileRandomForest(\n",
    "    numberOfTrees=25,\n",
    "    variablesPerSplit=2,\n",
    "    bagFraction=0.632\n",
    ").setOutputMode(modelType))\n",
    "\n",
    "rf_VP3 = ee.Feature(ee.Geometry.Point([0,0])).set('cName','rf_VP3','c',ee.Classifier.smileRandomForest(\n",
    "    numberOfTrees=25,\n",
    "    variablesPerSplit=3,\n",
    "    bagFraction=0.632\n",
    ").setOutputMode(modelType))\n",
    "\n",
    "rf_VP4 = ee.Feature(ee.Geometry.Point([0,0])).set('cName','rf_VP4','c',ee.Classifier.smileRandomForest(\n",
    "    numberOfTrees=25,\n",
    "    variablesPerSplit=4,\n",
    "    bagFraction=0.632\n",
    ").setOutputMode(modelType))\n",
    "\n",
    "rf_VP5 = ee.Feature(ee.Geometry.Point([0,0])).set('cName','rf_VP5','c',ee.Classifier.smileRandomForest(\n",
    "    numberOfTrees=25,\n",
    "    variablesPerSplit=5,\n",
    "    bagFraction=0.632\n",
    ").setOutputMode(modelType))\n",
    "\n",
    "rf_VP6 = ee.Feature(ee.Geometry.Point([0,0])).set('cName','rf_VP6','c',ee.Classifier.smileRandomForest(\n",
    "    numberOfTrees=25,\n",
    "    variablesPerSplit=6,\n",
    "    bagFraction=0.632\n",
    ").setOutputMode(modelType))\n",
    "\n",
    "rf_VP7 = ee.Feature(ee.Geometry.Point([0,0])).set('cName','rf_VP7','c',ee.Classifier.smileRandomForest(\n",
    "    numberOfTrees=25,\n",
    "    variablesPerSplit=7,\n",
    "    bagFraction=0.632\n",
    ").setOutputMode(modelType))\n",
    "\n",
    "rf_VP8 = ee.Feature(ee.Geometry.Point([0,0])).set('cName','rf_VP8','c',ee.Classifier.smileRandomForest(\n",
    "    numberOfTrees=25,\n",
    "    variablesPerSplit=8,\n",
    "    bagFraction=0.632\n",
    ").setOutputMode(modelType))\n",
    "\n",
    "rf_VP9 = ee.Feature(ee.Geometry.Point([0,0])).set('cName','rf_VP9','c',ee.Classifier.smileRandomForest(\n",
    "    numberOfTrees=25,\n",
    "    variablesPerSplit=9,\n",
    "    bagFraction=0.632\n",
    ").setOutputMode(modelType))\n",
    "\n",
    "rf_VP10 = ee.Feature(ee.Geometry.Point([0,0])).set('cName','rf_VP10','c',ee.Classifier.smileRandomForest(\n",
    "    numberOfTrees=25,\n",
    "    variablesPerSplit=10,\n",
    "    bagFraction=0.632\n",
    ").setOutputMode(modelType))\n",
    "\n",
    "# Wrap all of the models into a feature collection for function mapping\n",
    "classifierList = [rf_VP2,\n",
    "                rf_VP3,\n",
    "                rf_VP4,\n",
    "                rf_VP5,\n",
    "                rf_VP6,\n",
    "                rf_VP7,\n",
    "                rf_VP8,\n",
    "                rf_VP9,\n",
    "                rf_VP10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the folder to house the cross validation feature collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users/acottam/ETH_Global_Forest_Cover/CV_Colls_RegTest being created...\n",
      "Asset created!\n"
     ]
    }
   ],
   "source": [
    "# Turn the folder string into an assetID and perform the deletion\n",
    "assetIDToCreate_Folder = 'users/'+usernameFolderString+'/'+projectFolder+'/'+cvCollFolder\n",
    "print(assetIDToCreate_Folder,'being created...')\n",
    "\n",
    "# Create the folder within Earth Engine\n",
    "subprocess.run(bashCommandList_CreateFolder+[assetIDToCreate_Folder])\n",
    "while any(x in subprocess.run(bashCommandList_Detect+[assetIDToCreate_Folder],stdout=subprocess.PIPE).stdout.decode('utf-8') for x in stringsOfInterest):\n",
    "    print('Waiting for asset to be created...')\n",
    "    time.sleep(normalWaitTime)\n",
    "print('Asset created!')\n",
    "\n",
    "\n",
    "# Sleep to allow the server time to receive incoming requests\n",
    "time.sleep(normalWaitTime/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No need to compute categorical levels!\n",
      "The pyramiding policy will be 'mean'.\n",
      "The accuracy type used for cross validation will be 'coefficient of determination'(i.e., R^2).\n"
     ]
    }
   ],
   "source": [
    "# According to the model/data type (classification/categorical versus regression/continuous), change variables that are used in the rest of the script\n",
    "if modelType == 'CLASSIFICATION':\n",
    "    categoricalLevels = [int(n) for n in list(ee.Dictionary(fcOI.aggregate_histogram(classProperty)).keys().getInfo())]\n",
    "    print('Categorical levels are\\n')\n",
    "    print(categoricalLevels)\n",
    "    pyramidingPolicy = 'mode'\n",
    "    print(\"The pyramiding policy will be 'mode'.\")\n",
    "    accuracyMetricString = 'OverallAccuracy'\n",
    "    print(\"The accuracy type used for crossvalidation will be 'overall accuracy'.\")\n",
    "else:\n",
    "    print('No need to compute categorical levels!')\n",
    "    print(\"The pyramiding policy will be 'mean'.\")\n",
    "    pyramidingPolicy = 'mean'\n",
    "    print(\"The accuracy type used for cross validation will be 'coefficient of determination'(i.e., R^2).\")\n",
    "    accuracyMetricString = 'R2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the R^2 function for use with continuous valued models (i.e., regression based models)\n",
    "def coefficientOfDetermination(fcOI,propertyOfInterest,propertyOfInterest_Predicted):\n",
    "    # Compute the mean of the property of interest\n",
    "    propertyOfInterestMean = ee.Number(ee.Dictionary(ee.FeatureCollection(fcOI).select([propertyOfInterest]).reduceColumns(ee.Reducer.mean(),[propertyOfInterest])).get('mean'));\n",
    "    \n",
    "    # Compute the total sum of squares\n",
    "    def totalSoSFunction(f):\n",
    "        return f.set('Difference_Squared',ee.Number(ee.Feature(f).get(propertyOfInterest)).subtract(propertyOfInterestMean).pow(ee.Number(2)))\n",
    "    totalSumOfSquares = ee.Number(ee.Dictionary(ee.FeatureCollection(fcOI).map(totalSoSFunction).select(['Difference_Squared']).reduceColumns(ee.Reducer.sum(),['Difference_Squared'])).get('sum'))\n",
    "    \n",
    "    # Compute the residual sum of squares\n",
    "    def residualSoSFunction(f):\n",
    "        return f.set('Residual_Squared',ee.Number(ee.Feature(f).get(propertyOfInterest)).subtract(ee.Number(ee.Feature(f).get(propertyOfInterest_Predicted))).pow(ee.Number(2)))\n",
    "    residualSumOfSquares = ee.Number(ee.Dictionary(ee.FeatureCollection(fcOI).map(residualSoSFunction).select(['Residual_Squared']).reduceColumns(ee.Reducer.sum(),['Residual_Squared'])).get('sum'))\n",
    "    \n",
    "    # Finalize the calculation\n",
    "    r2 = ee.Number(1).subtract(residualSumOfSquares.divide(totalSumOfSquares))\n",
    "    \n",
    "    return ee.Number(r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and classify the cross validation feature collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a feature collection from the k-fold assignment list\n",
    "# !! Note: this is used within the scope of the function below, so this should be defined\n",
    "# !! explicitly in order for the computeCVAccuracy function to run\n",
    "kFoldAssignmentFC = ee.FeatureCollection(ee.List(kList).map(lambda n: ee.Feature(ee.Geometry.Point([0,0])).set('Fold',n)))\n",
    "\n",
    "# Define a function to take a feature with a classifier of interest\n",
    "def computeCVAccuracy(featureWithClassifier):\n",
    "    # Pull the classifier from the feature\n",
    "    cOI = ee.Classifier(featureWithClassifier.get('c'))\n",
    "    \n",
    "    # Create a function to map through the fold assignments and compute the overall accuracy \n",
    "    # for all validation folds\n",
    "    def computeAccuracyForFold(foldFeature):\n",
    "        # Organize the training and validation data\n",
    "        foldNumber = ee.Number(ee.Feature(foldFeature).get('Fold'))\n",
    "        trainingData = fcOI.filterMetadata(cvFoldString,'not_equals',foldNumber)\n",
    "        validationData = fcOI.filterMetadata(cvFoldString,'equals',foldNumber)\n",
    "        # Train the classifier and classify the validation dataset\n",
    "        trainedClassifier = cOI.train(trainingData,classProperty,covariateList)\n",
    "        outputtedPropName = classProperty+'_Predicted'\n",
    "        classifiedValidationData = validationData.classify(trainedClassifier,outputtedPropName)\n",
    "        # Create a central if/then statement that determines the type of accuracy values that are returned\n",
    "        if modelType == 'CLASSIFICATION':\n",
    "            # Compute the overall accuracy of the classification\n",
    "            errorMatrix = classifiedValidationData.errorMatrix(classProperty,outputtedPropName,categoricalLevels)\n",
    "            overallAccuracy = ee.Number(errorMatrix.accuracy())\n",
    "            return foldFeature.set(accuracyMetricString,overallAccuracy)\n",
    "        else:\n",
    "            # Compute the R^2 of the regression\n",
    "            r2ToSet = coefficientOfDetermination(classifiedValidationData,classProperty,outputtedPropName)\n",
    "            return foldFeature.set(accuracyMetricString,r2ToSet)\n",
    "    \n",
    "    # Compute the accuracy values of the classifier across all folds\n",
    "    accuracyFC = kFoldAssignmentFC.map(computeAccuracyForFold)\n",
    "    meanAccuracy = accuracyFC.aggregate_mean(accuracyMetricString)\n",
    "    tsdAccuracy = accuracyFC.aggregate_total_sd(accuracyMetricString)\n",
    "    \n",
    "    # Compute the feature to return\n",
    "    featureToReturn = featureWithClassifier.select(['cName']).set('Mean_'+accuracyMetricString,meanAccuracy,'StDev_'+accuracyMetricString,tsdAccuracy)\n",
    "    return featureToReturn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the classified features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users/acottam/ETH_Global_Forest_Cover/CV_Colls_RegTest/rf_VP2 started!\n",
      "users/acottam/ETH_Global_Forest_Cover/CV_Colls_RegTest/rf_VP3 started!\n",
      "users/acottam/ETH_Global_Forest_Cover/CV_Colls_RegTest/rf_VP4 started!\n",
      "users/acottam/ETH_Global_Forest_Cover/CV_Colls_RegTest/rf_VP5 started!\n",
      "users/acottam/ETH_Global_Forest_Cover/CV_Colls_RegTest/rf_VP6 started!\n",
      "users/acottam/ETH_Global_Forest_Cover/CV_Colls_RegTest/rf_VP7 started!\n",
      "users/acottam/ETH_Global_Forest_Cover/CV_Colls_RegTest/rf_VP8 started!\n",
      "users/acottam/ETH_Global_Forest_Cover/CV_Colls_RegTest/rf_VP9 started!\n",
      "users/acottam/ETH_Global_Forest_Cover/CV_Colls_RegTest/rf_VP10 started!\n",
      "All CV jobs queued; moving on...\n"
     ]
    }
   ],
   "source": [
    "# !! Export the accuracy FC's individually for memory purposes\n",
    "for featureWithClassifier in classifierList:\n",
    "    accuracyFC = ee.FeatureCollection(ee.Feature(computeCVAccuracy(featureWithClassifier)))\n",
    "    classifierName = str(featureWithClassifier.get('cName').getInfo())\n",
    "    finalClassifierFCExport = ee.batch.Export.table.toAsset(\n",
    "        collection=accuracyFC,\n",
    "        description=classifierName,\n",
    "        assetId='users/'+usernameFolderString+'/'+projectFolder+'/'+cvCollFolder+'/'+classifierName\n",
    "    );\n",
    "    finalClassifierFCExport.start()\n",
    "    print('users/'+usernameFolderString+'/'+projectFolder+'/'+cvCollFolder+'/'+classifierName+' started!')\n",
    "print('All CV jobs queued; moving on...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have jobs running!  2020-04-05 18:38:32\n",
      "You have jobs running!  2020-04-05 18:38:37\n",
      "You have jobs running!  2020-04-05 18:38:43\n",
      "You have jobs running!  2020-04-05 18:38:48\n",
      "You have jobs running!  2020-04-05 18:38:54\n",
      "You have jobs running!  2020-04-05 18:39:00\n",
      "You have jobs running!  2020-04-05 18:39:05\n",
      "You have jobs running!  2020-04-05 18:39:10\n",
      "You have jobs running!  2020-04-05 18:39:16\n",
      "You have jobs running!  2020-04-05 18:39:21\n",
      "You have jobs running!  2020-04-05 18:39:27\n",
      "You have jobs running!  2020-04-05 18:39:32\n",
      "You have jobs running!  2020-04-05 18:39:38\n",
      "You have jobs running!  2020-04-05 18:39:43\n",
      "You have jobs running!  2020-04-05 18:39:49\n",
      "You have jobs running!  2020-04-05 18:39:54\n",
      "You have jobs running!  2020-04-05 18:40:00\n",
      "You have jobs running!  2020-04-05 18:40:05\n",
      "You have jobs running!  2020-04-05 18:40:11\n",
      "You have jobs running!  2020-04-05 18:40:16\n",
      "You have jobs running!  2020-04-05 18:40:21\n",
      "You have jobs running!  2020-04-05 18:40:27\n",
      "You have jobs running!  2020-04-05 18:40:32\n",
      "You have jobs running!  2020-04-05 18:40:38\n",
      "You have jobs running!  2020-04-05 18:40:43\n",
      "You have jobs running!  2020-04-05 18:40:49\n",
      "You have jobs running!  2020-04-05 18:40:54\n",
      "You have jobs running!  2020-04-05 18:41:00\n",
      "You have jobs running!  2020-04-05 18:41:05\n",
      "You have jobs running!  2020-04-05 18:41:11\n",
      "You have jobs running!  2020-04-05 18:41:16\n",
      "You have jobs running!  2020-04-05 18:41:22\n",
      "You have jobs running!  2020-04-05 18:41:27\n",
      "You have jobs running!  2020-04-05 18:41:32\n",
      "You have jobs running!  2020-04-05 18:41:38\n",
      "You have jobs running!  2020-04-05 18:41:43\n",
      "You have jobs running!  2020-04-05 18:41:49\n",
      "You have jobs running!  2020-04-05 18:41:54\n",
      "You have jobs running!  2020-04-05 18:42:00\n",
      "You have jobs running!  2020-04-05 18:42:05\n",
      "You have jobs running!  2020-04-05 18:42:10\n",
      "You have jobs running!  2020-04-05 18:42:16\n",
      "You have jobs running!  2020-04-05 18:42:21\n",
      "You have jobs running!  2020-04-05 18:42:27\n",
      "You have jobs running!  2020-04-05 18:42:32\n",
      "You have jobs running!  2020-04-05 18:42:38\n",
      "You have jobs running!  2020-04-05 18:42:43\n",
      "You have jobs running!  2020-04-05 18:42:48\n",
      "You have jobs running!  2020-04-05 18:42:54\n",
      "You have jobs running!  2020-04-05 18:42:59\n",
      "You have jobs running!  2020-04-05 18:43:05\n",
      "You have jobs running!  2020-04-05 18:43:10\n",
      "You have jobs running!  2020-04-05 18:43:16\n",
      "You have jobs running!  2020-04-05 18:43:21\n",
      "You have jobs running!  2020-04-05 18:43:27\n",
      "You have jobs running!  2020-04-05 18:43:32\n",
      "You have jobs running!  2020-04-05 18:43:37\n",
      "You have jobs running!  2020-04-05 18:43:43\n",
      "You have jobs running!  2020-04-05 18:43:48\n",
      "You have jobs running!  2020-04-05 18:43:54\n",
      "You have jobs running!  2020-04-05 18:43:59\n",
      "You have jobs running!  2020-04-05 18:44:05\n",
      "You have jobs running!  2020-04-05 18:44:10\n",
      "You have jobs running!  2020-04-05 18:44:15\n",
      "You have jobs running!  2020-04-05 18:44:21\n",
      "You have jobs running!  2020-04-05 18:44:26\n",
      "You have jobs running!  2020-04-05 18:44:32\n",
      "You have jobs running!  2020-04-05 18:44:37\n",
      "You have jobs running!  2020-04-05 18:44:43\n",
      "You have jobs running!  2020-04-05 18:44:48\n",
      "You have jobs running!  2020-04-05 18:44:54\n",
      "You have jobs running!  2020-04-05 18:44:59\n",
      "You have jobs running!  2020-04-05 18:45:04\n",
      "You have jobs running!  2020-04-05 18:45:10\n",
      "You have jobs running!  2020-04-05 18:45:15\n",
      "You have jobs running!  2020-04-05 18:45:21\n",
      "You have jobs running!  2020-04-05 18:45:26\n",
      "You have jobs running!  2020-04-05 18:45:32\n",
      "You have jobs running!  2020-04-05 18:45:37\n",
      "You have jobs running!  2020-04-05 18:45:43\n",
      "You have jobs running!  2020-04-05 18:45:48\n",
      "You have jobs running!  2020-04-05 18:45:53\n",
      "You have jobs running!  2020-04-05 18:45:59\n",
      "You have jobs running!  2020-04-05 18:46:04\n",
      "You have jobs running!  2020-04-05 18:46:10\n",
      "Moving on...\n"
     ]
    }
   ],
   "source": [
    "# !! Break and wait\n",
    "while any(x in str(ee.batch.Task.list()) for x in ['RUNNING','READY']):\n",
    "    print('You have jobs running! ',datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    time.sleep(normalWaitTime)\n",
    "print('Moving on...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the features into a feature collection and export as an asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create/export a feature collection specifically to hold all of the accuracy values\n",
    "cvAccuracyFC = []\n",
    "for featureWithClassifier in classifierList:\n",
    "    cvAccuracyFC.append(ee.Feature(ee.FeatureCollection('users/'+usernameFolderString+'/'+projectFolder+'/'+cvCollFolder+'/'+str(featureWithClassifier.get('cName').getInfo())).first()))\n",
    "cvAccuracyFC = ee.FeatureCollection(cvAccuracyFC)\n",
    "\n",
    "cvAccuracyFCExport = ee.batch.Export.table.toAsset(\n",
    "    collection=cvAccuracyFC,\n",
    "    description=cvAccuracyFCNameString,\n",
    "    assetId='users/'+usernameFolderString+'/'+projectFolder+'/'+cvCollFolder+'/'+cvAccuracyFCNameString\n",
    ");\n",
    "cvAccuracyFCExport.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have jobs running!  2020-04-05 18:46:19\n",
      "You have jobs running!  2020-04-05 18:46:24\n",
      "You have jobs running!  2020-04-05 18:46:30\n",
      "You have jobs running!  2020-04-05 18:46:35\n",
      "Moving on...\n"
     ]
    }
   ],
   "source": [
    "# !! Break and wait\n",
    "while any(x in str(ee.batch.Task.list()) for x in ['RUNNING','READY']):\n",
    "    print('You have jobs running! ',datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    time.sleep(normalWaitTime)\n",
    "print('Moving on...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'FeatureCollection', 'columns': {'Mean_R2': 'Float', 'StDev_R2': 'Float', 'cName': 'String', 'system:index': 'String'}, 'version': 1586112398304843, 'id': 'users/acottam/ETH_Global_Forest_Cover/CV_Colls_RegTest/CV_Accuracy_FC_RegTest', 'properties': {'system:asset_size': 9055}, 'features': [{'type': 'Feature', 'geometry': {'type': 'Point', 'coordinates': [0, 0]}, 'id': '00000000000000000003', 'properties': {'Mean_R2': 0.5718027734974569, 'StDev_R2': 0.003659647133458033, 'cName': 'rf_VP5'}}, {'type': 'Feature', 'geometry': {'type': 'Point', 'coordinates': [0, 0]}, 'id': '00000000000000000001', 'properties': {'Mean_R2': 0.5713517965109007, 'StDev_R2': 0.0031138620284438764, 'cName': 'rf_VP3'}}, {'type': 'Feature', 'geometry': {'type': 'Point', 'coordinates': [0, 0]}, 'id': '00000000000000000000', 'properties': {'Mean_R2': 0.570778092745803, 'StDev_R2': 0.00402692397342983, 'cName': 'rf_VP2'}}, {'type': 'Feature', 'geometry': {'type': 'Point', 'coordinates': [0, 0]}, 'id': '00000000000000000004', 'properties': {'Mean_R2': 0.57068193301861, 'StDev_R2': 0.001302157160983837, 'cName': 'rf_VP6'}}, {'type': 'Feature', 'geometry': {'type': 'Point', 'coordinates': [0, 0]}, 'id': '00000000000000000006', 'properties': {'Mean_R2': 0.5694848255469096, 'StDev_R2': 0.002858560974554271, 'cName': 'rf_VP8'}}, {'type': 'Feature', 'geometry': {'type': 'Point', 'coordinates': [0, 0]}, 'id': '00000000000000000002', 'properties': {'Mean_R2': 0.5690532199018916, 'StDev_R2': 0.0024727640569691353, 'cName': 'rf_VP4'}}, {'type': 'Feature', 'geometry': {'type': 'Point', 'coordinates': [0, 0]}, 'id': '00000000000000000005', 'properties': {'Mean_R2': 0.5688230370281723, 'StDev_R2': 0.002939373298287893, 'cName': 'rf_VP7'}}, {'type': 'Feature', 'geometry': {'type': 'Point', 'coordinates': [0, 0]}, 'id': '00000000000000000007', 'properties': {'Mean_R2': 0.5677986032320523, 'StDev_R2': 0.0009978533936526655, 'cName': 'rf_VP9'}}, {'type': 'Feature', 'geometry': {'type': 'Point', 'coordinates': [0, 0]}, 'id': '00000000000000000008', 'properties': {'Mean_R2': 0.5664370367085809, 'StDev_R2': 0.002176747597575671, 'cName': 'rf_VP10'}}]}\n"
     ]
    }
   ],
   "source": [
    "# Print the full set of accuracy values within the feature collection\n",
    "print(ee.FeatureCollection('users/'+usernameFolderString+'/'+projectFolder+'/'+cvCollFolder+'/'+cvAccuracyFCNameString).sort('Mean_'+accuracyMetricString,False).getInfo())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model \n",
      " rf_VP5\n",
      "\n",
      "\n",
      "Model Cross Validated Mean R2 \n",
      " 0.5718027734974569\n",
      "\n",
      "\n",
      "Model Cross Validated Standard Deviation R2 \n",
      " 0.003659647133458033\n"
     ]
    }
   ],
   "source": [
    "# Print the info on the best model\n",
    "mostAccurateModelFeature = ee.Feature(ee.FeatureCollection('users/'+usernameFolderString+'/'+projectFolder+'/'+cvCollFolder+'/'+cvAccuracyFCNameString).sort('Mean_'+accuracyMetricString,False).sort('Mean_'+accuracyMetricString,False).first())\n",
    "bestModelName = mostAccurateModelFeature.get('cName').getInfo()\n",
    "bestModelMeanAccuracy = mostAccurateModelFeature.get('Mean_'+accuracyMetricString).getInfo()\n",
    "bestModelAccuracyStDev = mostAccurateModelFeature.get('StDev_'+accuracyMetricString).getInfo()\n",
    "print('Best Model','\\n',bestModelName)\n",
    "print('\\n')\n",
    "print('Model Cross Validated Mean '+accuracyMetricString,'\\n',bestModelMeanAccuracy)\n",
    "print('\\n')\n",
    "print('Model Cross Validated Standard Deviation '+accuracyMetricString,'\\n',bestModelAccuracyStDev)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping collection creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0     25999\n",
       "13.0    14443\n",
       "11.0    14367\n",
       "1.0      6233\n",
       "7.0      4559\n",
       "10.0     2599\n",
       "5.0      1992\n",
       "4.0      1473\n",
       "12.0      489\n",
       "8.0       184\n",
       "9.0        79\n",
       "2.0        75\n",
       "14.0       29\n",
       "3.0         3\n",
       "Name: Resolve_Biome, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of points within each biome\n",
    "preppedCollection['Resolve_Biome'].value_counts()\n",
    "# results = preppedCollection['Resolve_Biome'].value_counts()\n",
    "# df1 = results.to_frame()\n",
    "# resolveBiomesDict = [[1, 'Tropical and subtropical moist broadleaf forests'],\n",
    "# [2,  'Tropical and subtropical dry broadleaf forests'],\n",
    "# [3, 'Tropical and subtropical coniferous forests'],\n",
    "# [4, 'Temperate broadleaf and mixed forests'],\n",
    "# [5, 'Temperate conifer forests'],\n",
    "# [6, 'Boreal forests or taiga'],\n",
    "# [7, 'Tropical and subtropical grasslands, savannas, and shrublands'],\n",
    "# [8, 'Temperate grasslands, savannas, and shrublands'],\n",
    "# [9, 'Flooded Grasslands and Savannas'],\n",
    "# [10, 'Montane grasslands and shrublands'],\n",
    "# [11, 'Tundra'],\n",
    "# [12, 'Mediterranean forests, woodlands, and scrub'],\n",
    "# [13, 'Deserts and xeric shrublands'],\n",
    "# [14, 'Mangroves']]\n",
    "# df2 = pd.DataFrame(resolveBiomesDict, columns=['id','name']).set_index('id')\n",
    "# df1 = df1.join(df2)\n",
    "# print(df1[['name','Resolve_Biome']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input the dictionary of values for each of the biomes\n",
    "# This was computed using the following script\n",
    "# https://code.earthengine.google.com/d98223f98f6f11073aa21b059bf667d6\n",
    "# Missing assests in that script - not sure what it does\n",
    "# These look like percentages!! Maybe its the area of biome as a percentage?\n",
    "biomeDict = {\n",
    "    1: 14.900835665820974,\n",
    "    2: 2.941697660221864,\n",
    "    3: 0.526059731441294,\n",
    "    4: 9.56387696566245,\n",
    "    5: 2.865354077500338,\n",
    "    6: 11.519674266872787,\n",
    "    7: 16.26999434439293,\n",
    "    8: 8.047078485979089,\n",
    "    9: 0.861212221078014,\n",
    "    10: 3.623974712557433,\n",
    "    11: 6.063922959332467,\n",
    "    12: 2.5132866428302836,\n",
    "    13: 20.037841544639985,\n",
    "    14: 0.26519072167008,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CHELSA_Annual_Mean_Temperature', 'CHELSA_Annual_Precipitation', 'CHELSA_Mean_Temperature_of_Warmest_Quarter', 'CHELSA_Precipitation_Seasonality', 'CHELSA_Precipitation_of_Driest_Quarter', 'CV_Fold', 'EarthEnvTopoMed_Eastness', 'EarthEnvTopoMed_Elevation', 'EarthEnvTopoMed_Northness', 'Lat', 'Long', 'Resolve_Biome', 'SG_Depth_to_bedrock', 'SG_Sand_Content_000cm', 'SG_Sand_Content_005cm', '__annotations__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_accessors', '_add_numeric_operations', '_agg_examples_doc', '_agg_see_also_doc', '_aggregate', '_aggregate_frame', '_aggregate_item_by_item', '_aggregate_multiple_funcs', '_apply_filter', '_apply_to_column_groupbys', '_apply_whitelist', '_assure_grouper', '_bool_agg', '_builtin_table', '_choose_path', '_concat_objects', '_constructor', '_cumcount_array', '_cython_agg_blocks', '_cython_agg_general', '_cython_table', '_cython_transform', '_define_paths', '_deprecations', '_dir_additions', '_dir_deletions', '_fill', '_get_cython_func', '_get_cythonized_result', '_get_data_to_aggregate', '_get_index', '_get_indices', '_gotitem', '_group_selection', '_insert_inaxis_grouper_inplace', '_internal_names', '_internal_names_set', '_is_builtin_func', '_iterate_column_groupbys', '_iterate_slices', '_make_wrapper', '_obj_with_exclusions', '_python_agg_general', '_python_apply_general', '_reindex_output', '_reset_cache', '_reset_group_selection', '_selected_obj', '_selection', '_selection_list', '_selection_name', '_set_group_selection', '_set_result_index_ordered', '_transform_fast', '_transform_general', '_transform_item_by_item', '_transform_should_cast', '_try_aggregate_string_function', '_try_cast', '_wrap_agged_blocks', '_wrap_aggregated_output', '_wrap_applied_output', '_wrap_frame_output', '_wrap_transformed_output', 'agg', 'aggregate', 'all', 'any', 'apply', 'backfill', 'bfill', 'boxplot', 'corr', 'corrwith', 'count', 'cov', 'cumcount', 'cummax', 'cummin', 'cumprod', 'cumsum', 'describe', 'diff', 'dtypes', 'expanding', 'ffill', 'fillna', 'filter', 'first', 'get_group', 'groups', 'head', 'hist', 'idxmax', 'idxmin', 'indices', 'last', 'mad', 'max', 'mean', 'median', 'min', 'ndim', 'ngroup', 'ngroups', 'nth', 'nunique', 'ohlc', 'pad', 'pct_change', 'pipe', 'plot', 'prod', 'quantile', 'rank', 'resample', 'rolling', 'sem', 'shift', 'shrubcover', 'size', 'skew', 'std', 'sum', 'tail', 'take', 'transform', 'treecover', 'tshift', 'var']\n"
     ]
    }
   ],
   "source": [
    "print(dir(preppedCollection.groupby('Resolve_Biome', group_keys=False)))\n",
    "# print(preppedCollection.groupby('Resolve_Biome', group_keys=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CHELSA_Annual_Mean_Temperature', 'CHELSA_Annual_Precipitation',\n",
      "       'CHELSA_Mean_Temperature_of_Warmest_Quarter',\n",
      "       'CHELSA_Precipitation_Seasonality',\n",
      "       'CHELSA_Precipitation_of_Driest_Quarter', 'EarthEnvTopoMed_Eastness',\n",
      "       'EarthEnvTopoMed_Elevation', 'EarthEnvTopoMed_Northness', 'Lat', 'Long',\n",
      "       'Resolve_Biome', 'SG_Depth_to_bedrock', 'SG_Sand_Content_000cm',\n",
      "       'SG_Sand_Content_005cm', 'shrubcover', 'treecover', 'CV_Fold'],\n",
      "      dtype='object')\n",
      "(0, 17)\n"
     ]
    }
   ],
   "source": [
    "# Perform an example stratified sample by biome\n",
    "# AC This is pandas code - why not use GEE?\n",
    "# print(dir(preppedCollection.groupby('Resolve_Biome', group_keys=False)))\n",
    "# print(preppedCollection.groupby('Resolve_Biome', group_keys=False).first())\n",
    "stratSample = preppedCollection.groupby('Resolve_Biome', group_keys=False).apply(lambda x: x.sample(n=int(round((biomeDict.get(x.name)/100)*bootstrapModelSize)), replace=True, random_state=1))\n",
    "print(stratSample.columns)\n",
    "print(stratSample.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ForestPotential2_BootstrapColl_RegTest_001 uploaded to a GCSB!\n",
      "ForestPotential2_BootstrapColl_RegTest_002 uploaded to a GCSB!\n",
      "ForestPotential2_BootstrapColl_RegTest_003 uploaded to a GCSB!\n",
      "ForestPotential2_BootstrapColl_RegTest_004 uploaded to a GCSB!\n",
      "ForestPotential2_BootstrapColl_RegTest_005 uploaded to a GCSB!\n",
      "CPU times: user 157 ms, sys: 32 ms, total: 189 ms\n",
      "Wall time: 6.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run a for loop to create multiple bootstrap iterations and upload them to the Google Cloud Storage Bucket\n",
    "\n",
    "# Create an empty list to store all of the file name strings being uploaded (for later use)\n",
    "fileNameList = []\n",
    "\n",
    "for n in seedsToUseForBootstrapping:\n",
    "    # Perform the subsetting\n",
    "    stratSample = preppedCollection.groupby('Resolve_Biome', group_keys=False).apply(lambda x: x.sample(n=int(round((biomeDict.get(x.name)/100)*bootstrapModelSize)), replace=True, random_state=n))\n",
    "    \n",
    "    # Format the title of the CSV and export it to a holding location\n",
    "    titleOfBootstrapCSV = fileNameHeader+str(n).zfill(3)\n",
    "    fileNameList.append(titleOfBootstrapCSV)\n",
    "    fullLocalPath = holdingFolder+'/'+titleOfBootstrapCSV+'.csv'\n",
    "    stratSample.to_csv(holdingFolder+'/'+titleOfBootstrapCSV+'.csv',index=False)\n",
    "    \n",
    "    # Format the bash call to upload the files to the Google Cloud Storage bucket\n",
    "    gsutilBashUploadList = [bashFunctionGSUtil]+arglist_preGSUtilUploadFile+[fullLocalPath]+[formattedBucketOI]\n",
    "    subprocess.run(gsutilBashUploadList)\n",
    "    print(titleOfBootstrapCSV+' uploaded to a GCSB!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything is uploaded; moving on...\n"
     ]
    }
   ],
   "source": [
    "# Wait for the GSUTIL uploading process to finish before moving on\n",
    "while not all(x in subprocess.run([bashFunctionGSUtil,'ls',formattedBucketOI],stdout=subprocess.PIPE).stdout.decode('utf-8') for x in fileNameList):\n",
    "    print('Not everything is uploaded...')\n",
    "    time.sleep(5)\n",
    "print('Everything is uploaded; moving on...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users/acottam/ETH_Global_Forest_Cover/Bootstrap_Collections_RegTest being created...\n",
      "Asset created!\n"
     ]
    }
   ],
   "source": [
    "# Create a folder to house the bootstrapped feature collection\n",
    "\n",
    "# Turn the folder string into an assetID and perform the deletion\n",
    "assetIDToCreate_Folder = 'users/'+usernameFolderString+'/'+projectFolder+'/'+bootstrapCollFolder\n",
    "print(assetIDToCreate_Folder,'being created...')\n",
    "\n",
    "# Create the image collection before classifying each of the bootstrap images\n",
    "subprocess.run(bashCommandList_CreateFolder+[assetIDToCreate_Folder])\n",
    "while any(x in subprocess.run(bashCommandList_Detect+[assetIDToCreate_Folder], stdout=subprocess.PIPE).stdout.decode('utf-8') for x in stringsOfInterest):\n",
    "    print('Waiting for asset to be created...')\n",
    "    time.sleep(normalWaitTime)\n",
    "print('Asset created!')\n",
    "\n",
    "\n",
    "# Sleep to allow the server time to receive incoming requests\n",
    "time.sleep(normalWaitTime/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ForestPotential2_BootstrapColl_RegTest_001 EarthEngine Ingestion started!\n",
      "ForestPotential2_BootstrapColl_RegTest_002 EarthEngine Ingestion started!\n",
      "ForestPotential2_BootstrapColl_RegTest_003 EarthEngine Ingestion started!\n",
      "ForestPotential2_BootstrapColl_RegTest_004 EarthEngine Ingestion started!\n",
      "ForestPotential2_BootstrapColl_RegTest_005 EarthEngine Ingestion started!\n",
      "All files are being ingested.\n"
     ]
    }
   ],
   "source": [
    "# Loop through the file names and upload each of them to Earth Engine\n",
    "for f in fileNameList:\n",
    "    assetIDForBootstrapColl = 'users/'+usernameFolderString+'/'+projectFolder+'/'+bootstrapCollFolder\n",
    "    gsStorageFileLocation = formattedBucketOI\n",
    "    earthEngineUploadTableCommands = [bashFunction_EarthEngine]+arglist_preEEUploadTable+[assetIDStringPrefix+assetIDForBootstrapColl+'/'+f]+[gsStorageFileLocation+'/'+f+'.csv']+arglist_postEEUploadTable\n",
    "    subprocess.run(earthEngineUploadTableCommands)\n",
    "    print(f+' EarthEngine Ingestion started!')\n",
    "print('All files are being ingested.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have jobs running!  2020-04-05 18:47:30\n",
      "You have jobs running!  2020-04-05 18:47:35\n",
      "You have jobs running!  2020-04-05 18:47:41\n",
      "You have jobs running!  2020-04-05 18:47:47\n",
      "Moving on...\n"
     ]
    }
   ],
   "source": [
    "# !! Break and wait\n",
    "while any(x in str(ee.batch.Task.list()) for x in ['RUNNING','READY']):\n",
    "    print('You have jobs running! ',datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    time.sleep(normalWaitTime)\n",
    "print('Moving on...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the best model, load then train all of the bootstrapped collections and make maps with each of them within an assigned image collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users/acottam/ETH_Global_Forest_Cover/Bootstrap_Images_RegTest being created...\n",
      "Asset created!\n"
     ]
    }
   ],
   "source": [
    "# Create an image collection to house the outputted bootstrapped images\n",
    "\n",
    "# Turn the folder string into an assetID and perform the creation\n",
    "assetIDToCreate_Collection = 'users/'+usernameFolderString+'/'+projectFolder+'/'+bootstrapImageColl\n",
    "print(assetIDToCreate_Collection,'being created...')\n",
    "\n",
    "# Create the image collection before classifying each of the bootstrap images\n",
    "subprocess.run(bashCommandList_CreateCollection+[assetIDToCreate_Collection])\n",
    "while any(x in subprocess.run(bashCommandList_Detect+[assetIDToCreate_Collection],stdout=subprocess.PIPE).stdout.decode('utf-8') for x in stringsOfInterest):\n",
    "    print('Waiting for asset to be created...')\n",
    "    time.sleep(normalWaitTime)\n",
    "print('Asset created!')\n",
    "\n",
    "\n",
    "# Sleep to allow the server time to receive incoming requests\n",
    "time.sleep(normalWaitTime/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "EEException",
     "evalue": "Collection.loadTable: Collection asset 'users/acottam/ETH_Global_Forest_Cover/Bootstrap_Collections_RegTest/ForestPotential2_BootstrapColl_RegTest_001' not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda2/envs/ee/lib/python3.8/site-packages/ee/data.py\u001b[0m in \u001b[0;36m_execute_cloud_call\u001b[0;34m(call, num_retries)\u001b[0m\n\u001b[1;32m    337\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_retries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_retries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mapiclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHttpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ee/lib/python3.8/site-packages/googleapiclient/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ee/lib/python3.8/site-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHttpError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHttpError\u001b[0m: <HttpError 400 when requesting https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/image:export?alt=json returned \"Collection.loadTable: Collection asset 'users/acottam/ETH_Global_Forest_Cover/Bootstrap_Collections_RegTest/ForestPotential2_BootstrapColl_RegTest_001' not found.\">",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEEException\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-614ebddedc55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mpyramidingPolicy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\".default\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpyramidingPolicy\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     );\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mbootstrapImageExport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitleOfColl\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' queued!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ee/lib/python3.8/site-packages/ee/batch.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXPORT_IMAGE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexportImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXPORT_MAP\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexportMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ee/lib/python3.8/site-packages/ee/data.py\u001b[0m in \u001b[0;36mexportImage\u001b[0;34m(request_id, params)\u001b[0m\n\u001b[1;32m   1311\u001b[0m   \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_use_cloud_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1313\u001b[0;31m     return _prepare_and_run_export(\n\u001b[0m\u001b[1;32m   1314\u001b[0m         request_id, params, _cloud_api_resource.projects().image().export)\n\u001b[1;32m   1315\u001b[0m   \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'EXPORT_IMAGE'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ee/lib/python3.8/site-packages/ee/data.py\u001b[0m in \u001b[0;36m_prepare_and_run_export\u001b[0;34m(request_id, params, export_endpoint)\u001b[0m\n\u001b[1;32m   1435\u001b[0m         params['expression'], for_cloud_api=True)\n\u001b[1;32m   1436\u001b[0m   \u001b[0mnum_retries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAX_RETRIES\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrequest_id\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1437\u001b[0;31m   return _execute_cloud_call(\n\u001b[0m\u001b[1;32m   1438\u001b[0m       \u001b[0mexport_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_get_projects_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1439\u001b[0m       num_retries=num_retries)\n",
      "\u001b[0;32m~/miniconda2/envs/ee/lib/python3.8/site-packages/ee/data.py\u001b[0m in \u001b[0;36m_execute_cloud_call\u001b[0;34m(call, num_retries)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_retries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_retries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mapiclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHttpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0m_translate_cloud_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEEException\u001b[0m: Collection.loadTable: Collection asset 'users/acottam/ETH_Global_Forest_Cover/Bootstrap_Collections_RegTest/ForestPotential2_BootstrapColl_RegTest_001' not found."
     ]
    }
   ],
   "source": [
    "# Load the best model from the classifier list\n",
    "classifierToBootstrap = ee.Classifier(ee.Feature(ee.FeatureCollection(classifierList).filterMetadata('cName','equals',bestModelName).first()).get('c'))\n",
    "\n",
    "# Run a for loop to create multiple bootstrap iterations\n",
    "for n in seedsToUseForBootstrapping:\n",
    "    \n",
    "    # Format the title of the CSV and export it to a holding location\n",
    "    titleOfColl = fileNameHeader+str(n).zfill(3)\n",
    "    collectionPath = 'users/'+usernameFolderString+'/'+projectFolder+'/'+bootstrapCollFolder+'/'+titleOfColl\n",
    "    \n",
    "    # Load the collection from the path\n",
    "    fcToTrain = ee.FeatureCollection(collectionPath)\n",
    "    \n",
    "    # Train the classifier with the collection\n",
    "    trainedClassifer = classifierToBootstrap.train(fcToTrain,classProperty,covariateList)\n",
    "    \n",
    "    # Classify the image\n",
    "    classifiedImage = compositeToClassify.classify(trainedClassifer,classProperty+'_Predicted')\n",
    "    \n",
    "    # Queue the export\n",
    "    # !! The current pyramiding policy is set to mode as the current map is categorical;\n",
    "    # !! Make sure to change this argument when dealing with continuous datasets\n",
    "    bootstrapImageExport = ee.batch.Export.image.toAsset(\n",
    "        image=classifiedImage,\n",
    "        description=titleOfColl,\n",
    "        assetId=assetIDToCreate_Collection+'/'+titleOfColl,\n",
    "        crs='EPSG:4326',\n",
    "        crsTransform='[0.008333333333333333,0,-180,0,-0.008333333333333333,90]',\n",
    "        region=exportingGeometry.getInfo()['coordinates'],\n",
    "        maxPixels=int(1e13),\n",
    "        pyramidingPolicy={\".default\": pyramidingPolicy}\n",
    "    );\n",
    "    bootstrapImageExport.start()\n",
    "    print(titleOfColl+' queued!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !! Break and wait\n",
    "# while any(x in str(ee.batch.Task.list()) for x in ['RUNNING','READY']):\n",
    "#     print('You have jobs running! ',datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "#     time.sleep(longWaitTime)\n",
    "# print('Moving on...')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
